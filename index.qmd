---
title: "Algorithmic Bias in AI Resume Screening"
author: 
  name: "Aidan Bugayong"
  affiliations:
    name: "Department of Mathematics, Applied Mathematics, and Statistics, Case Western Reserve University"
    city: "Cleveland"
    state: "Ohio"
date: "2025-11-14"
title-block-banner: true
execute:
  echo: false
  warning: false
toc: true
format: 
  pdf: 
    documentclass: report
    geometry:
      - top=30mm
      - left=20mm
      - right=20mm
      - heightrounded
    mainfont: Times New Roman
    fontsize: 12pt
    colorlinks: true
    date-format: long
  html:
    toc: true
website:
  favicon: "images\\CWRU.jpg"
---
```{python}
import os
import pandas as pd
import re

data_folder = "data"

def clean_ethnicity(ethnicity_str, known_ethnicities):
    """
    Clean ethnicity entries:
    - If the entry contains any known ethnicity, extract and return that ethnicity
    - Otherwise, return 'Unknown'
    """
    if pd.isna(ethnicity_str):
        return 'Unknown'
    
    ethnicity_str = str(ethnicity_str).strip()
    
    # Check if the string contains any of the known ethnicities
    for ethnicity in known_ethnicities:
        if ethnicity.lower() in ethnicity_str.lower():
            return ethnicity
    
    # If no known ethnicity found, return 'Unknown'
    return 'Unknown'

def clean_dataframe(df):
    """Clean the dataframe by removing HTML tags and fixing categorical values"""
    
    # Define known ethnicities
    known_ethnicities = ['Hispanic', 'African American', 'Unknown', 'Caucasian', 'Asian']
    
    # Drop NA values
    df = df.dropna()
    
    # Clean all string columns
    for col in df.select_dtypes(include=['object']):
        # Remove HTML tags and extra whitespace
        df[col] = df[col].astype(str).apply(lambda x: re.sub(r'<.*?>', '', x)).str.strip()
        
        # Column-specific cleaning
        if col == 'gender':
            # Fix Male/Female entries and standardize values
            gender_mapping = {
                'Male/Female': 'Unknown',
                'M': 'Male',
                'F': 'Female',
                'male': 'Male', 
                'female': 'Female',
                'unknown': 'Unknown',
                'Other': 'Unknown',
                'Male': 'Male',
                'Female': 'Female'
            }
            df[col] = df[col].replace(gender_mapping)
            # Set any remaining unexpected values to Unknown
            valid_genders = ['Male', 'Female', 'Unknown']
            df[col] = df[col].apply(lambda x: x if x in valid_genders else 'Unknown')
            
        elif col == 'ethnicity':
            # Clean ethnicity entries - extract known ethnicities from explanations
            df[col] = df[col].apply(lambda x: clean_ethnicity(x, known_ethnicities))
            
        elif col == 'prestige':
            # Clean prestige entries  
            df[col] = df[col].str.title()
    
    return df

for filename in os.listdir(data_folder):
    if filename.endswith("_resume_scores.csv"):
        model_name = filename.replace("_resume_scores.csv", "")
        input_path = os.path.join(data_folder, filename)
        output_path = os.path.join(data_folder, f"{model_name}.csv")

        try:
            # Read and clean the data
            df = pd.read_csv(input_path)
            df_clean = clean_dataframe(df)
            
            # Save cleaned data
            df_clean.to_csv(output_path, index=False)
                
        except Exception as e:
            print(f"✗ Error processing {filename}: {e}")

```

# 1 Introduction
## 1.1 Context and Background
Resume screeners were developed in order to screen canidates more efficiently and reduce the human bias in the screening process. However, there are concerns with whether or not these automated resume screening systems are truly unbiased in their decision making process. As more and more companies use some form of AI automation in their hiring process, the question of whether or not these systems are unbiased has become more important. 

According to Naveen Kumar in his article on AI recruitment statistics, roughly 87% of companies are using AI in the hiring and recruitment process[^NaveenKumar]. As such, the University of Washington decided to conduct their own study to determine if there is any bias in the decision making process of resume screeners and what sorts of factors contribute to the scoring process of a resume. Their research found "significant racial, gender and intersectional bias in how three state-of-the-art large language models, or LLMs, ranked resumes. The researchers varied names associated with white and Black men and women across over 550 real-world resumes and found the LLMs favored white-associated names 85% of the time, female-associated names only 11% of the time, and never favored Black male-associated names over white male-associated names"[^Washington]. Their research came to these conclusions by handing the AI a list of identical resumes with different names and then having the AI give them scores. The article ends with the research team noting that more research should be done in this area by looking at different attributes and more LLMs in order to better allign these AI systems with the real world policies to reduce bias and harm.

## 1.2 Project Purpose
This project aims to determine if there is any bias in the decision making process of resume screeners and what sorts of factors contribute to the scoring process of a resume. More specifically, looking at a wide variety of applicant attributes to determine what factors have the highest contribution to the bias in the decision making process. Ideally, professional experience and education will be the best predictors of the resume scores but we will also look into other factors such as gender, ethnicity, and institutional prestige.

# 2 Methods
## 2.1 Data Collection and Purpose
The original list of resumes is from a dataset in huggingface[^huggingface], which is comprised of both real and synthetic resume data in JSON formatting. The purpose of this dataset was for training natural language processing (NLP) models for resume parsing. Specifically, the resumes are oriented around technical roles and is designed for NLP models to be trained on and used for candidate matching / screening in this field. This dataset was posted on huggingface Feburary 21st, 2025 and has not been updated since (excluding the minor changes to the readme).

The sources used for this dataset are sourced from anonymized CV submissions as well as synthetiic resumes generated using "Faker Library" and filled with realistic and role appropriate information. All resumes are anonymized by removing PII (Personally Identifiable Information) but many fields (such as names) contain realistic placeholders. The makers of the dataset note that the data is oriented around technical roles and the synthetic resumes may not capture the same nuances of a real resume. As such, the makers note that this dataset should only be used for NLP, data augmentation, or exploratory data analysis and should not be used for non-technical roles or personalized hiring decisions.

The dataset contains over 4500 resumes in a JSON format. Each resume entry contains personal information, work experience, education information, skills and projects. Since these are technical resumes (oriented around the computer science / information technology field), the skills and projects fields contained a candidate's coding projects and/or coding languages. For the scope of this project, all fields were used for analysis or scoring of the resume. 

## 2.2 Data Processing
The collection of resumes was processed in order to create the score datasets used for this project. The score datasets has the following columns: Name (acting as the primary key), Resume Score, Gender, Ethnicity, Institutional Prestige, Years of Experience, Skill Relevance, Experience Relevance and Project Relevance. The score datasets are specific to each of the langauge models used in this project. This is because we are trying to understand the relation between how the model scores a reseume and the model's own perception of the candidate (gender, ethnicity, and institutional prestige).

For creating the score datasets themselves, the resumes were scored by the model and then the model was asked to guess the gender, ethnicity, and institutional prestige of the candidate. For determining the skill/project/expperience relevance, that was also a call to the language model. It is important to note that each column of data is a separate instance of the language model to reduce the liklihood of previous responses affecting the current responses. All of this information was then used to create the score dataset for each of the models used in this project. 

The models used for this project include IBM's *granite3.3:8b*, Microsoft's *Phi4:14b* and Meta's *llama3.1:8b*. These models were choosen because they are highly rated models despite the lower paramter size and are all open source. 

## 2.3 Statistical Tools and Approach
### 2.3a Exploratory Data Analysis
Before 

### 2.3b Initial Modeling Approach
The initial approach was to use linear regression to predict the resume score from all of the other variables in the dataset. The formula used for this model was: `score ~ gender + ethnicity + prestige + skill_score + project_score + experience_score + years_experience`. Here, the response variable is the resume score which are integers in the range 0-100. The categorical variables are: gender, ethnicity, and prestige. The numeric variables are: years of experience, skill score, project score, and experience score.

### 2.3c Model Diagnostics 

### 2.3d Model Refinement

# 3 Results and Discussion
```{python}
import statsmodels.formula.api as smf
import scipy.stats as stats
from statsmodels.stats.outliers_influence import variance_inflation_factor
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

#| fig-cap: "Histogram of Scores"

# Set plotting style
plt.style.use('bmh')
# sns.set_palette("husl")

# =============================================================================
# SECTION 1: DATA LOADING AND PREPROCESSING
# =============================================================================

def load_and_clean_data():
    datasets = {}
    
    datasets['ibm'] = pd.read_csv("data/granite3.3_8b.csv")
    datasets['meta'] = pd.read_csv("data/llama3.1_8b.csv")
    datasets['microsoft'] = pd.read_csv("data/phi4_14b.csv")
    
    return datasets

def preprocess_data(datasets):
    def set_categorical_order(df):
        if 'gender' in df.columns:
            df['gender'] = pd.Categorical(df['gender'], 
                                        categories=['Unknown', 'Male', 'Female'], 
                                        ordered=False)
        
        if 'ethnicity' in df.columns:
            df['ethnicity'] = pd.Categorical(df['ethnicity'],
                                           categories=['Unknown', 'Caucasian', 'African American', 'Asian', 'Hispanic'],
                                           ordered=False)
        
        if 'prestige' in df.columns:
            # Set specific order for prestige
            prestige_order = ['Unknown', 'Low', 'Medium', 'High']
            existing_cats = [cat for cat in prestige_order if cat in df['prestige'].unique()]
            df['prestige'] = pd.Categorical(df['prestige'], 
                                          categories=existing_cats,
                                          ordered=True)
        return df
    
    for name, df in datasets.items():
        datasets[name] = set_categorical_order(df)
    return datasets

# Load and preprocess data
datasets = load_and_clean_data()
datasets = preprocess_data(datasets)

# Create merged dataframe with Company column
merged_data = []
company_mapping = {'ibm': 'IBM', 'meta': 'Meta', 'microsoft': 'Microsoft'}

for name, df in datasets.items():
    df_copy = df.copy()
    df_copy['Company'] = company_mapping.get(name, name.title())
    merged_data.append(df_copy)

merged_df = pd.concat(merged_data, ignore_index=True)

# Create histograms using sns.histplot
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Distribution of Scores Across Models', fontsize=16, fontweight='bold')

columns_to_plot = ['score', 'skill_score', 'project_score', 'experience_score']
axes_flat = axes.flatten()

# Use fixed bins from 0 to 100 in increments of 4
fixed_bins = np.arange(0, 104, 4)  # 0, 4, 8, ..., 100

for idx, col in enumerate(columns_to_plot):
    ax = axes_flat[idx]
    
    # Create histogram using sns.histplot
    sns.histplot(data=merged_df, x=col, hue='Company', bins=50, 
                 ax=ax, edgecolor='black', alpha=0.7, stat='count')
    
    ax.set_xlabel(col.replace('_', ' ').title(), fontweight='bold')
    ax.set_ylabel('Frequency', fontweight='bold')
    ax.set_title(f'Distribution of {col.replace("_", " ").title()}', fontweight='bold')
    ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

```

```{python}
# Create pie charts for categorical variables
categorical_vars = ['gender', 'ethnicity', 'prestige']

for cat_var in categorical_vars:
    if cat_var not in merged_df.columns:
        continue
    
    # Create 1x3 grid for the three models
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    fig.suptitle(f'{cat_var.title()} Distribution by Model', fontsize=16, fontweight='bold')
    
    companies = merged_df['Company'].unique()
    
    for idx, company in enumerate(sorted(companies)):
        ax = axes[idx]
        
        # Filter data for this company
        company_data = merged_df[merged_df['Company'] == company]
        value_counts = company_data[cat_var].value_counts()
        
        # Create pie chart without labels
        ax.pie(value_counts.values, autopct='%1.1f%%',
               startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})
        ax.set_title(f'{company}', fontweight='bold', fontsize=12)
    
    # Add legend to the rightmost pie chart
    axes[-1].legend(value_counts.index, loc='upper left', bbox_to_anchor=(1, 1),
                    fontsize=10, frameon=True, title=cat_var.title())
    
    plt.tight_layout()
    plt.show()

'''
# Generate covariance matrices for each dataset
for name, df in datasets.items():
    # Select numeric columns for covariance
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    cov_matrix = df[numeric_cols].cov()
    # Visualize covariance matrix as heatmap
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(cov_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, 
                square=True, linewidths=1, cbar_kws={"shrink": 0.8}, ax=ax)
    ax.set_title(f'Covariance Matrix - {name.title()}', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()
'''
```

```{python}
# =============================================================================
# SECTION 2: INITIAL MODEL DEVELOPMENT
# =============================================================================

print("\nSECTION 2: INITIAL MODEL DEVELOPMENT")
print("=" * 50)

def create_initial_models(datasets):
    """Create initial linear regression models"""
    models = {}
    
    formula = 'score ~ gender + ethnicity + prestige + skill_score + project_score + experience_score + years_experience'
    
    for name, data in datasets.items():
        try:
            models[name] = smf.ols(formula, data=data).fit()
            print(f"✓ Created initial model for {name}")
            print(f"  R-squared: {models[name].rsquared:.4f}")
        except Exception as e:
            print(f"✗ Error creating model for {name}: {e}")
    
    return models

# Create initial models
initial_models = create_initial_models(datasets)

```

```{python}
# =============================================================================
# SECTION 3: INTERACTION EFFECTS ANALYSIS
# =============================================================================

print("\nSECTION 3: INTERACTION EFFECTS ANALYSIS")
print("=" * 50)

def test_interaction_effects(datasets):
    """Test for significant interaction effects"""
    interaction_results = {}
    
    # Define interaction pairs to test
    interaction_pairs = [
        ('gender', 'ethnicity'),
        ('ethnicity', 'prestige'),
        ('prestige', 'gender')
    ]
    
    for name, data in datasets.items():
        print(f"\nTesting interactions for {name}:")
        interaction_results[name] = {}
        
        for var1, var2 in interaction_pairs:
            # Create model with interaction term
            interaction_formula = f'score ~ gender + ethnicity + prestige + skill_score + project_score + experience_score + years_experience + {var1}:{var2}'
            
            try:
                model_with_interaction = smf.ols(interaction_formula, data=data).fit()
                
                # Compare with model without interaction using F-test
                base_formula = 'score ~ gender + ethnicity + prestige + skill_score + project_score + experience_score + years_experience'
                base_model = smf.ols(base_formula, data=data).fit()
                
                # F-test for interaction term
                f_test = model_with_interaction.compare_f_test(base_model)
                p_value = f_test[1]
                
                interaction_results[name][f"{var1}:{var2}"] = {
                    'p_value': p_value,
                    'significant': p_value < 0.05,
                    'r_squared': model_with_interaction.rsquared
                }
                
                significance = "✓ SIGNIFICANT" if p_value < 0.05 else "not significant"
                print(f"  {var1} × {var2}: p = {p_value:.4f} ({significance})")
                
            except Exception as e:
                print(f"  Error testing {var1} × {var2}: {e}")
    
    return interaction_results

# Test interaction effects
interaction_results = test_interaction_effects(datasets)

def create_interaction_visualizations(datasets, interaction_pairs):
    """Create comprehensive interaction visualizations"""
    
    for company, data in datasets.items():
        # Create main interaction plot with 1x3 layout
        fig, axes = plt.subplots(1, 3, figsize=(15, 8))
        fig.suptitle(f'Interaction Effects Analysis - {company.title()}', 
                    fontsize=16, fontweight='bold')
        
        # Flatten axes for easier indexing
        axes_flat = axes.flatten()
        
        # Plot each interaction pair
        for idx, (var1, var2) in enumerate(interaction_pairs):
            if idx >= len(axes_flat):
                break
                
            ax = axes_flat[idx]
            
            # Create interaction heatmap
            pivot_table = data.groupby([var1, var2])['score'].mean().unstack()
            
            # Create annotated heatmap
            im = ax.imshow(pivot_table.values, cmap='RdYlBu', aspect='auto', 
                          vmin=data['score'].min(), vmax=data['score'].max())
            
            # Set labels
            ax.set_xticks(range(len(pivot_table.columns)))
            ax.set_xticklabels(pivot_table.columns, rotation=45, ha='right')
            ax.set_yticks(range(len(pivot_table.index)))
            ax.set_yticklabels(pivot_table.index)
            
            # Add value annotations
            for i in range(len(pivot_table.index)):
                for j in range(len(pivot_table.columns)):
                    ax.text(j, i, f'{pivot_table.iloc[i, j]:.1f}', 
                           ha='center', va='center', fontweight='bold',
                           fontsize=9, color='white' if pivot_table.iloc[i, j] < pivot_table.values.mean() else 'black')
            
            ax.set_xlabel(var2.title(), fontweight='bold')
            ax.set_ylabel(var1.title(), fontweight='bold')
            ax.set_title(f'{var1.title()} × {var2.title()} Interaction', 
                        fontweight='bold', fontsize=12)
        
        plt.tight_layout()
        plt.show()

# Create interaction visualizations
interaction_pairs = [('gender', 'ethnicity'), ('ethnicity', 'prestige'), ('prestige', 'gender')]
create_interaction_visualizations(datasets, interaction_pairs)
```

```{python}
# =============================================================================
# SECTION 4: VARIABLE SELECTION AND MODEL REFINEMENT
# =============================================================================

print("\nSECTION 4: VARIABLE SELECTION AND MODEL REFINEMENT")
print("=" * 50)

def refine_models(datasets, initial_models):
    """Refine models by removing non-significant variables with proper formula handling"""
    refined_models = {}
    model_comparisons = {}
    
    for name, initial_model in initial_models.items():
        print(f"\nRefining model for {name}:")
        
        # Get significant variables (p < 0.1)
        significant_vars = initial_model.pvalues[initial_model.pvalues < 0.1].index.tolist()
        
        # Remove intercept from significant variables
        if 'Intercept' in significant_vars:
            significant_vars.remove('Intercept')
        
        # Convert encoded categorical variables back to original variable names
        cleaned_vars = []
        categorical_base_vars = set()
        
        for var in significant_vars:
            # Check if this is an encoded categorical variable (contains [T. or [])
            if '[' in var and ']' in var:
                # Extract the base variable name (e.g., 'gender[T.Female]' -> 'gender')
                base_var = var.split('[')[0]
                categorical_base_vars.add(base_var)
            else:
                # Continuous variable or non-encoded categorical
                cleaned_vars.append(var)
        
        # Add base categorical variables (not the encoded versions)
        cleaned_vars.extend(list(categorical_base_vars))
        
        # If too few significant variables, keep top variables by p-value
        if len(cleaned_vars) < 2:
            print("  Too few significant variables, using top variables by p-value")
            pvalues = initial_model.pvalues.drop('Intercept', errors='ignore')
            # Get top variables, excluding encoded categoricals
            top_vars = []
            for var in pvalues.nsmallest(5).index:
                if '[' in var and ']' in var:
                    base_var = var.split('[')[0]
                    if base_var not in top_vars:
                        top_vars.append(base_var)
                else:
                    if var != 'Intercept':
                        top_vars.append(var)
            cleaned_vars = top_vars[:3]  # Take top 3
        
        # Build refined formula
        if cleaned_vars:
            # Use the original variable names, not the encoded ones
            refined_formula = f"score ~ {' + '.join(cleaned_vars)}"
        else:
            refined_formula = "score ~ 1"  # Intercept only
        
        print(f"  Refined formula: {refined_formula}")
        
        try:
            refined_model = smf.ols(refined_formula, data=datasets[name]).fit()
            refined_models[name] = refined_model
            
            # Compare models
            aic_improvement = initial_model.aic - refined_model.aic
            rsq_improvement = refined_model.rsquared - initial_model.rsquared
            
            model_comparisons[name] = {
                'initial_vars': len(initial_model.params),
                'refined_vars': len(refined_model.params),
                'initial_aic': initial_model.aic,
                'refined_aic': refined_model.aic,
                'aic_improvement': aic_improvement,
                'initial_rsq': initial_model.rsquared,
                'refined_rsq': refined_model.rsquared,
                'rsq_improvement': rsq_improvement
            }
            
            print(f"  Initial AIC: {initial_model.aic:.2f}")
            print(f"  Refined AIC: {refined_model.aic:.2f}")
            print(f"  AIC Improvement: {aic_improvement:+.2f}")
            print(f"  Variables: {len(initial_model.params)} → {len(refined_model.params)}")
            print(f"  Refined R-squared: {refined_model.rsquared:.4f}")
            
        except Exception as e:
            print(f"  Error refining model: {e}")
            print(f"  Keeping original model for {name}")
            refined_models[name] = initial_model
    
    return refined_models, model_comparisons

# Refine models
refined_models, model_comparisons = refine_models(datasets, initial_models)

def create_model_comparison_plot(model_comparisons):
    """Create visualization comparing initial and refined models"""
    
    companies = list(model_comparisons.keys())
    metrics = ['initial_aic', 'refined_aic']
    labels = ['Initial Model', 'Refined Model']
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # AIC comparison
    x_pos = np.arange(len(companies))
    width = 0.35
    
    for i, metric in enumerate(metrics):
        values = [model_comparisons[company][metric] for company in companies]
        ax1.bar(x_pos + i*width, values, width, label=labels[i], alpha=0.8)
    
    ax1.set_xlabel('Company')
    ax1.set_ylabel('AIC Value')
    ax1.set_title('AIC Comparison: Initial vs Refined Models', fontweight='bold')
    ax1.set_xticks(x_pos + width/2)
    ax1.set_xticklabels([c.title() for c in companies])
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # R-squared comparison
    initial_rsq = [model_comparisons[company]['initial_rsq'] for company in companies]
    refined_rsq = [model_comparisons[company]['refined_rsq'] for company in companies]
    
    x_pos = np.arange(len(companies))
    ax2.bar(x_pos - width/2, initial_rsq, width, label='Initial', alpha=0.8)
    ax2.bar(x_pos + width/2, refined_rsq, width, label='Refined', alpha=0.8)
    
    ax2.set_xlabel('Company')
    ax2.set_ylabel('R-squared')
    ax2.set_title('R-squared Comparison: Initial vs Refined Models', fontweight='bold')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels([c.title() for c in companies])
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

create_model_comparison_plot(model_comparisons)

```

```{python}
# =============================================================================
# SECTION 5: MODEL DIAGNOSTICS AND ASSUMPTIONS
# =============================================================================

print("\nSECTION 5: MODEL DIAGNOSTICS AND ASSUMPTIONS")
print("=" * 50)

def check_model_assumptions(models, datasets):
    """Comprehensive model diagnostics and assumption checking"""
    
    assumption_results = {}
    
    for name, model in models.items():
        print(f"\nChecking assumptions for {name} model:")
        assumption_results[name] = {}
        
        # Get residuals and fitted values
        residuals = model.resid
        fitted = model.fittedvalues
        n = len(residuals)
        
        # 1. Normality test (Shapiro-Wilk)
        if n < 5000:  # Shapiro-Wilk is recommended for n < 5000
            shapiro_stat, shapiro_p = stats.shapiro(residuals)
            assumption_results[name]['normality_p'] = shapiro_p
            normality = "✓ Normal" if shapiro_p > 0.05 else "✗ Non-normal"
            print(f"  Normality (Shapiro-Wilk): p = {shapiro_p:.4f} ({normality})")
        
        # 2. Homoscedasticity (Breusch-Pagan test)
        try:
            from statsmodels.stats.diagnostic import het_breuschpagan
            bp_lm, bp_p, _, _ = het_breuschpagan(residuals, model.model.exog)
            assumption_results[name]['homoscedasticity_p'] = bp_p
            homoscedastic = "✓ Homoscedastic" if bp_p > 0.05 else "✗ Heteroscedastic"
            print(f"  Homoscedasticity (Breusch-Pagan): p = {bp_p:.4f} ({homoscedastic})")
        except:
            print("  Homoscedasticity test failed")
        
        # 3. Multicollinearity (VIF)
        try:
            # Calculate VIF for each variable
            vif_data = pd.DataFrame()
            vif_data["Variable"] = model.model.exog_names
            vif_data["VIF"] = [variance_inflation_factor(model.model.exog, i) 
                             for i in range(model.model.exog.shape[1])]
            
            high_vif = vif_data[vif_data['VIF'] > 10]
            assumption_results[name]['high_vif_count'] = len(high_vif)
            print(f"  Multicollinearity: {len(high_vif)} variables with VIF > 10")
            
        except:
            print("  VIF calculation failed")
        
        # 4. Outlier detection
        Q1 = residuals.quantile(0.25)
        Q3 = residuals.quantile(0.75)
        IQR = Q3 - Q1
        outlier_threshold = 1.5 * IQR
        outliers = residuals[(residuals < (Q1 - outlier_threshold)) | 
                           (residuals > (Q3 + outlier_threshold))]
        assumption_results[name]['outlier_count'] = len(outliers)
        print(f"  Outliers: {len(outliers)} potential outliers detected")
    
    return assumption_results

# Check model assumptions
assumption_results = check_model_assumptions(refined_models, datasets)

def create_diagnostic_plots(models, datasets):
    """Create comprehensive diagnostic plots with proper data alignment"""
    
    for name, model in models.items():
        print(f"\nCreating diagnostic plots for {name}...")
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle(f'Model Diagnostic Plots - {name.title()}', 
                    fontsize=16, fontweight='bold')
        
        residuals = model.resid
        fitted = model.fittedvalues
        
        # Get the actual data used in the model (after dropping NA)
        # model.model.data.orig_exog contains the original exogenous data
        # but we need to align it properly
        try:
            # Method 1: Get the row indices used in the model
            if hasattr(model.model, 'row_labels'):
                used_indices = model.model.row_labels
                actual_scores = datasets[name].iloc[used_indices]['score']
            else:
                # Method 2: Use the endogenous variable from the model
                actual_scores = model.model.endog
        except:
            # Method 3: Fallback - try to align by length
            if len(fitted) == len(datasets[name]):
                actual_scores = datasets[name]['score']
            else:
                # Method 4: Last resort - use fitted values length and assume it's a subset
                print(f"  Warning: Data alignment issue for {name}. Using available data.")
                actual_scores = datasets[name]['score'].iloc[:len(fitted)]
        
        # Ensure we have the same number of observations
        if len(actual_scores) != len(fitted):
            print(f"  Data mismatch: {len(actual_scores)} actual scores vs {len(fitted)} fitted values")
            # Use the minimum length to avoid errors
            min_len = min(len(actual_scores), len(fitted))
            actual_scores = actual_scores[:min_len]
            fitted_adj = fitted[:min_len]
            residuals_adj = residuals[:min_len]
        else:
            fitted_adj = fitted
            residuals_adj = residuals
        
        # 1. Residuals vs Fitted
        axes[0, 0].scatter(fitted_adj, residuals_adj, alpha=0.6, color='steelblue')
        axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)
        axes[0, 0].set_xlabel('Fitted Values')
        axes[0, 0].set_ylabel('Residuals')
        axes[0, 0].set_title('Residuals vs Fitted Values')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Q-Q Plot
        stats.probplot(residuals_adj, dist="norm", plot=axes[0, 1])
        axes[0, 1].set_title('Q-Q Plot: Normality Check')
        
        # 3. Scale-Location Plot (sqrt standardized residuals vs fitted)
        standardized_residuals = np.sqrt(np.abs(residuals_adj / residuals_adj.std()))
        axes[0, 2].scatter(fitted_adj, standardized_residuals, alpha=0.6, color='green')
        axes[0, 2].set_xlabel('Fitted Values')
        axes[0, 2].set_ylabel('√|Standardized Residuals|')
        axes[0, 2].set_title('Scale-Location Plot')
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. Residuals Distribution
        axes[1, 0].hist(residuals_adj, bins=20, alpha=0.7, color='purple', edgecolor='black')
        axes[1, 0].axvline(x=0, color='red', linestyle='--', alpha=0.8)
        axes[1, 0].set_xlabel('Residuals')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Distribution of Residuals')
        
        # 5. Actual vs Predicted
        axes[1, 1].scatter(actual_scores, fitted_adj, alpha=0.6, color='orange')
        min_val = min(actual_scores.min(), fitted_adj.min())
        max_val = max(actual_scores.max(), fitted_adj.max())
        axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'red', linestyle='--', alpha=0.8)
        axes[1, 1].set_xlabel('Actual Scores')
        axes[1, 1].set_ylabel('Predicted Scores')
        axes[1, 1].set_title('Actual vs Predicted Values')
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. Cook's Distance for influential points
        try:
            from statsmodels.stats.outliers_influence import OLSInfluence
            influence = OLSInfluence(model)
            cooks_d = influence.cooks_distance[0]
            axes[1, 2].stem(range(len(cooks_d)), cooks_d, basefmt=" ")
            axes[1, 2].axhline(y=4/len(cooks_d), color='red', linestyle='--', alpha=0.8, 
                             label='4/n threshold')
            axes[1, 2].set_xlabel('Observation Index')
            axes[1, 2].set_ylabel("Cook's Distance")
            axes[1, 2].set_title("Cook's Distance for Influential Points")
            axes[1, 2].legend()
            axes[1, 2].grid(True, alpha=0.3)
        except Exception as e:
            axes[1, 2].text(0.5, 0.5, f"Cook's Distance\nCalculation Failed\n{str(e)}", 
                          ha='center', va='center', transform=axes[1, 2].transAxes)
            axes[1, 2].set_title("Cook's Distance")
        
        plt.tight_layout()
        plt.show()

create_diagnostic_plots(refined_models, datasets)

```

```{python}
# =============================================================================
# SECTION 6: FINAL MODEL INTERPRETATION AND COMPARISON
# =============================================================================

print("\nSECTION 6: FINAL MODEL INTERPRETATION AND COMPARISON")
print("=" * 50)

def create_final_model_summary(refined_models):
    """Create comprehensive final model summary and visualizations"""
    
    # Create coefficient comparison plot
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Final Model Coefficient Estimates Across Companies', 
                fontsize=16, fontweight='bold')
    
    axes = axes.flatten()
    
    for idx, (company, model) in enumerate(refined_models.items()):
        if idx >= len(axes):
            break
            
        # Extract coefficients and confidence intervals
        coefficients = model.params
        conf_intervals = model.conf_int(alpha=0.05)  # 95% CI
        conf_intervals.columns = ['lower', 'upper']
        
        # Create DataFrame for plotting (exclude intercept)
        coef_df = pd.DataFrame({
            'coefficient': coefficients,
            'lower': conf_intervals['lower'],
            'upper': conf_intervals['upper']
        }).drop('Intercept', errors='ignore')
        
        # Sort by coefficient magnitude
        coef_df = coef_df.reindex(coef_df['coefficient'].abs().sort_values(ascending=False).index)
        
        # Create coefficient plot
        y_pos = np.arange(len(coef_df))
        axes[idx].errorbar(coef_df['coefficient'], y_pos, 
                          xerr=[coef_df['coefficient'] - coef_df['lower'], 
                                coef_df['upper'] - coef_df['coefficient']],
                          fmt='o', color='steelblue', alpha=0.8, capsize=5, 
                          markersize=6, linewidth=2)
        
        axes[idx].axvline(x=0, color='red', linestyle='--', alpha=0.7, linewidth=1)
        axes[idx].set_yticks(y_pos)
        axes[idx].set_yticklabels([label.replace('_', ' ').title() for label in coef_df.index], 
                                fontsize=9)
        axes[idx].set_xlabel('Coefficient Estimate', fontweight='bold')
        axes[idx].set_title(f'{company.title()}\nR² = {model.rsquared:.3f}', 
                          fontweight='bold', fontsize=12)
        axes[idx].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Print model summaries
    print("\nFINAL MODEL SUMMARIES:")
    print("-" * 60)
    
    for company, model in refined_models.items():
        print(f"\n{company.upper()} Final Model:")
        print(f"  R-squared: {model.rsquared:.4f}")
        print(f"  Adjusted R-squared: {model.rsquared_adj:.4f}")
        print(f"  F-statistic: {model.fvalue:.2f}")
        print(f"  Prob (F-statistic): {model.f_pvalue:.4f}")
        print(f"  AIC: {model.aic:.2f}")
        print(f"  BIC: {model.bic:.2f}")
        print(f"  Number of observations: {model.nobs}")
        
        # Print significant coefficients
        significant_coefs = model.pvalues[model.pvalues < 0.05].sort_values()
        if len(significant_coefs) > 0:
            print("  Significant predictors (p < 0.05):")
            for var, pval in significant_coefs.items():
                coef = model.params[var]
                print(f"    {var}: {coef:+.4f} (p = {pval:.4f})")
        else:
            print("  No significant predictors at p < 0.05")

def create_variable_importance_plot(models):
    """Create variable importance comparison across all models"""
    
    importance_data = []
    
    for company, model in models.items():
        # Use absolute t-values as measure of importance
        t_values = np.abs(model.tvalues.drop('Intercept', errors='ignore'))
        
        for var, t_val in t_values.items():
            coef = model.params[var]
            importance_data.append({
                'Company': company.title(),
                'Variable': var.replace('_', ' ').title(),
                'Importance': t_val,
                'Coefficient': coef,
                'Direction': 'Positive' if coef > 0 else 'Negative'
            })
    
    importance_df = pd.DataFrame(importance_data)
    
    # Create plot
    plt.figure(figsize=(14, 8))
    
    # Pivot for heatmap
    pivot_df = importance_df.pivot(index='Variable', columns='Company', values='Importance')
    
    # Sort by average importance
    pivot_df['Average'] = pivot_df.mean(axis=1)
    pivot_df = pivot_df.sort_values('Average', ascending=False)
    pivot_df = pivot_df.drop('Average', axis=1)
    
    # Create annotated heatmap
    sns.heatmap(pivot_df, annot=True, fmt='.2f', cmap='YlOrRd', 
                cbar_kws={'label': 'Absolute t-value (Importance)'},
                linewidths=0.5, linecolor='white')
    
    plt.title('Variable Importance Comparison Across Final Models', 
             fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Company', fontweight='bold')
    plt.ylabel('Predictor Variables', fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    return importance_df

# Create final summaries and visualizations
create_final_model_summary(refined_models)
importance_df = create_variable_importance_plot(refined_models)

# =============================================================================
# SUMMARY STATISTICS
# =============================================================================

print("\n" + "=" * 60)
print("ANALYSIS COMPLETE - SUMMARY")
print("=" * 60)

print(f"\nDatasets analyzed: {list(datasets.keys())}")
print(f"Total models created: {len(refined_models)}")

# Calculate overall performance metrics
avg_rsquared = np.mean([model.rsquared for model in refined_models.values()])
avg_adj_rsquared = np.mean([model.rsquared_adj for model in refined_models.values()])

print(f"\nAverage R-squared across models: {avg_rsquared:.4f}")
print(f"Average Adjusted R-squared across models: {avg_adj_rsquared:.4f}")

# Identify most consistent predictors
if not importance_df.empty:
    variable_consistency = importance_df.groupby('Variable')['Importance'].mean().sort_values(ascending=False)
    print(f"\nMost consistently important predictors:")
    for var, importance in variable_consistency.head(5).items():
        print(f"  {var}: {importance:.2f}")

print("\n✓ All analysis complete! Check generated plots for visualizations.")
```

# 4 Conclusion

# 5 References

[^NaveenKumar]: https://www.demandsage.com/ai-recruitment-statistics/

[^Washington]: https://www.washington.edu/news/2024/10/31/ai-bias-resume-screening-race-gender/

[^huggingface]: https://huggingface.co/datasets/datasetmaster/resumes
